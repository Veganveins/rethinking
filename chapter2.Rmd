---
title: "Chapter2"
output: html_document
---

A parameter is conjectured proportion of blue marbles (p). The likelihood: relative number of ways that a value p can produce the data (probability of the data). Likelihood is a formula that specifies plausibility of data. A parameter is the same as a  prior. For every parameter you intend your bayesian machine to estimate, you must provide to the  machine a prior. Priors are assumptions that constrain parameters to reasonable ranges. Beyond all of the above, there’s no law mandating we use only one prior. If you don’t have a strong argument for any particular prior, then try different ones. Because the prior is an assumption, it should be interrogated like other assumptions: by altering it and checking how sensitive inference is to the assumption. If the prior can be anything, isn't it possible to get any answer you want? Yes it is.

Grid approximation.
```{r}
# define grid
p_grid <- seq( from=0 , to=1 , length.out=20 )
# define prior
prior <- rep( 1 , 20 )
prior <- exp( -5*abs( p_grid - 0.5 ) )
# compute likelihood at each value in grid
likelihood <- dbinom( 6 , size=9 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

plot( p_grid , posterior , type="b" ,
      xlab="probability of water" , ylab="posterior probability" )
mtext( "20 points" )

```

Quadratic apporximation.
```{r}
globe.qa <- quap(
  alist(
    W ~ dbinom(W + L, p), # a binomial likelihood
    p ~ dunif(0,1) # a uniform prior
  ),
  
  data = list(W=24, L=12)
)
precis(globe.qa)

# Assuming the posterior distribution is guassian, it is maximized
# at .67, and its standard deviation is 0.16.

# analytical calculation
W = 24
L = 12
curve(dbeta(x , W+1, L+1), from = 0, to = 1)
# quadratic approximation
curve(dnorm(x, 0.67, 0.08), lty=2, add = TRUE)

```


Markov-chain Monte Carlo (MCMC)
```{r}
n_samples <- 1000
p <- rep( NA , n_samples )
p[1] <- 0.5
W <- 6
L <- 3
for ( i in 2:n_samples ) {
  p_new <- rnorm( 1 , p[i-1] , 0.1 )
  if ( p_new < 0 ) p_new <- abs( p_new )
  if ( p_new > 1 ) p_new <- 2 - p_new
  q0 <- dbinom( W , W+L , p[i-1] )
  q1 <- dbinom( W , W+L , p_new )
  p[i] <- ifelse( runif(1) < q1/q0 , p_new , p[i-1] )
}

dens( p , xlim=c(0,1) )
curve( dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE )

```

More mechanically, a Bayesian model is a composite of variables and distributional definitions for these variables. The probability of the data, often called the likelihood, provides the plausibility of an observation (data), given a fixed value for the parameters. The prior provides the plausibility of each possible value of the parameters, before accounting for the data. The rules of probability tell us that the logical way to compute the plausibilities, after accounting for the data, is to use Bayes’ theorem. This results in the posterior distribution.

#Homework Problems

## Medium Questions


#### 2M3. 
Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land. Further suppose that one of these globes—you don’t know which—was tossed in the air and produced a “land” observation. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing “land” (Pr(Earth|land)), is 0.23.

```{r}
# prob earth and mars
p_e = .5
p_m = .5

# prob land given earth or mars
p_lm = 1
p_le = .3

# prob land
p_l = p_e * p_le + p_m *p_lm

#probabality of earth given land
p_egl = p_le * p_e / p_l

p_egl
```

#### 2M4. 
Suppose you have a deck with only three cards. Each card has two sides, and each side is eitherblack or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you don’t know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black side facing up
on the table).

```{r }
c1 = 2
c2 = 1
c3 = 0

p_c1 = c1 / (c1 + c2 + c3)
p_c1
```

#### 2M5.
Now suppose there are four cards: B/B, B/W, W/W, and another B/B. Again suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black.
```{r}
bb = 4
bw = 1
ww = 0
 
p_bb = bb / (bb + bw + ww)
p_bb
```

#### 2M6.
Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, it’s less likely that a card with black sides is pulled from the bag. So again assume there are three cards: B/B, B/W, and W/W. After experimenting a number of times, you conclude that for every way to pull the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways to pull the W/W card. Again suppose that a card is pulled and a black side appears face up. Show that
the probability the other side is black is now 0.5. Use the counting method, as before.

```{r}
bb = 2
bw = 1
ww = 0


p_bb = bb / (bb + bw*2 + ww*3)
p_bb
```

#### 2M7
Assume again the original card problem, with a single card showing a black side face up. Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the first card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. Hint: Treat this like the sequence of globe tosses, counting all the ways to see each observation, for each possible first card.

```{r}
bb = 2
bw = 1
ww = 0

# the first card was bw and the second card is ww (2 ways this could happen)
# the first card was bb and second was bw (2 ways this could happen)
# first card was bb and second was ww (4 ways this could happen)

p_first_card_black = 6 / 8
p_first_card_black

```

## Hard Questions

#### 2H1
Suppose there are two species of panda bear. Both are equally common in the wild and live in the same places. They look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. They differ however in their family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise birthing singleton infants. Assume these numbers are known with certainty, from many years of field
research. Now suppose you are managing a captive panda breeding program. You have a new female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth
will also be twins?

```{r}
# initial probability of species
p_sa = .5
p_sb = .5

# probability of twins given species 
p_tga = .1
p_tgb = .2

# probability of twins
pt = .5*.1 + .5*.2

# probability of species A given twins
p_agt = p_tga * p_sa / pt
p_bgt = p_tgb * p_sb / pt


p_agt
p_bgt
# updated probability the unknown panda is species A = .33
# updated probability the unknown panda is species B = .66

updated_pt = .66*.2 + .33*.1
updated_pt
```

####2H3.
Continuing on from the previous problem, suppose the same panda mother has a second birth
and that it is not twins, but a singleton infant. Compute the posterior probability that this panda is species A.

```{r}
# before the panda has a singleton, we believe there is a 33% chance the panda is species A
# before the pandas has a singleton, we believe that the probability of a singleton infant is then equal to:

p_singleton = .33 * .9 + .66 * .8

#posterior_probability that the pandas is species A given a singleton at the second birth
# equales probability of singleton given a divided by p_singleton
p_ags = .9 * .33  / p_singleton
p_ags

# this is saying after seeing a singleton birth, we are 3% "more sure" that this is species A than before we saw 
# the singleton birth
```


####2H4
So suppose now that a veterinarian comes along who has a new genetic test that she claims can
identify the species of our mother panda. But the test, like all tests, is imperfect. This is the information you have about the test:
• The probability it correctly identifies a species A panda is 0.8.
• The probability it correctly identifies a species B panda is 0.65.
The vet administers the test to your panda and tells you that the test is positive for species A. First
ignore your previous information from the births and compute the posterior probability that your
panda is species A. Then redo your calculation, now using the birth data as well.

```{r}
# the test could have incorrectly identified a species B panda as A (with p = .35)
# or the test could have correctly identified a species A panda as A (with p = .8)

# probability that the panda is species A given an "A" test result is equal to the probability that the 

p_result_a_given_a = .8
p_a = .5
p_result_a = .8*.5 + .35*.5

p_a_given_result_a = p_result_a_given_a * p_a / p_result_a
p_a_given_result_a

```








